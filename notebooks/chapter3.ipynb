{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://aws:****@annaliseai-274616382064.d.codeartifact.ap-southeast-2.amazonaws.com/pypi/pypi/simple/\n",
      "Requirement already satisfied: nibabel in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (5.2.1)\n",
      "Requirement already satisfied: matplotlib in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: packaging>=17 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nibabel) (23.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://annaliseai-274616382064.d.codeartifact.ap-southeast-2.amazonaws.com/pypi/pypi/simple/pip/\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nibabel matplotlib numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"Convert images in sample to Tensors\"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self, sample: tuple[np.ndarray, np.ndarray]\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        src, tgt = sample\n",
    "        return torch.from_numpy(src).float(), torch.from_numpy(tgt).float()\n",
    "\n",
    "\n",
    "class RandomCrop3D:\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 3\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[0], sample[1]\n",
    "\n",
    "        h, w, d = image.shape[:3]\n",
    "        new_h, new_w, new_d = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h + 1)\n",
    "        left = np.random.randint(0, w - new_w + 1)\n",
    "        depth = np.random.randint(0, d - new_d + 1)\n",
    "\n",
    "        image = image[top : top + new_h, left : left + new_w, depth : depth + new_d]\n",
    "        label = label[top : top + new_h, left : left + new_w, depth : depth + new_d]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, source_dir: str, target_dir: str, transforms=None):\n",
    "        self.source_images = [\n",
    "            self._get_image_data(\"t1\", image)\n",
    "            for image in sorted(os.listdir(source_dir))\n",
    "        ]\n",
    "        self.target_images = [\n",
    "            self._get_image_data(\"t2\", image)\n",
    "            for image in sorted(os.listdir(target_dir))\n",
    "        ]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_images)\n",
    "\n",
    "    def _get_image_data(self, image_type: str, study_id: str):\n",
    "        image = nib.load(f\"../small/{image_type}/{study_id}\")\n",
    "        return image.get_fdata()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_data = (self.source_images[idx], self.target_images[idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            image_data = self.transforms(image_data)\n",
    "\n",
    "        return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "\n",
    "output_size = (42, 42, 42)\n",
    "train_data = NiftiDataset(\n",
    "    source_dir=\"../small/t1\",\n",
    "    target_dir=\"../small/t2\",\n",
    "    transforms=Compose([RandomCrop3D(output_size=output_size), ToTensor()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 42, 42])\n",
      "torch.Size([42, 42, 42])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][0].shape)\n",
    "print(train_data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, image_size: tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 42, 42, 42])\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(image_size=output_size)\n",
    "first_image = train_data[0][0].unsqueeze(0)\n",
    "\n",
    "predictions = model(first_image)\n",
    "print(predictions.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
