{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://aws:****@annaliseai-274616382064.d.codeartifact.ap-southeast-2.amazonaws.com/pypi/pypi/simple/\n",
      "Requirement already satisfied: nibabel in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (5.2.1)\n",
      "Requirement already satisfied: matplotlib in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: packaging>=17 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nibabel) (23.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/syamiyer/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nibabel matplotlib numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"Convert images in sample to Tensors\"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self, sample: tuple[np.ndarray, np.ndarray]\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        src, tgt = sample\n",
    "        return torch.from_numpy(src).float(), torch.from_numpy(tgt).float()\n",
    "\n",
    "\n",
    "class RandomCrop3D:\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 3\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[0], sample[1]\n",
    "\n",
    "        h, w, d = image.shape[:3]\n",
    "        new_h, new_w, new_d = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h + 1)\n",
    "        left = np.random.randint(0, w - new_w + 1)\n",
    "        depth = np.random.randint(0, d - new_d + 1)\n",
    "\n",
    "        image = image[top : top + new_h, left : left + new_w, depth : depth + new_d]\n",
    "        label = label[top : top + new_h, left : left + new_w, depth : depth + new_d]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, source_dir: str, target_dir: str, transforms=None):\n",
    "        self.source_images = [\n",
    "            self._get_image_data(\"t1\", image)\n",
    "            for image in sorted(os.listdir(source_dir))\n",
    "        ]\n",
    "        self.target_images = [\n",
    "            self._get_image_data(\"t2\", image)\n",
    "            for image in sorted(os.listdir(target_dir))\n",
    "        ]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_images)\n",
    "\n",
    "    def _get_image_data(self, image_type: str, study_id: str):\n",
    "        image = nib.load(f\"../small/{image_type}/{study_id}\")\n",
    "        return image.get_fdata()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_data = (self.source_images[idx], self.target_images[idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            image_data = self.transforms(image_data)\n",
    "\n",
    "        return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "\n",
    "output_size = (32, 32, 32)\n",
    "train_data = NiftiDataset(\n",
    "    source_dir=\"../small/t1\",\n",
    "    target_dir=\"../small/t2\",\n",
    "    transforms=Compose([RandomCrop3D(output_size=output_size), ToTensor()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 32])\n",
      "torch.Size([32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][0].shape)\n",
    "print(train_data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def conv(in_channels: int, out_channels: int):\n",
    "    return (\n",
    "        nn.Conv3d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        ),\n",
    "        nn.BatchNorm3d(num_features=out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "def double_conv(in_channels: int, intermediate_channels: int, out_channels: int):\n",
    "    return nn.Sequential(\n",
    "        *conv(in_channels=in_channels, out_channels=intermediate_channels),\n",
    "        *conv(in_channels=intermediate_channels, out_channels=out_channels)\n",
    "    )\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_size: tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "\n",
    "        # input shape = (1, image_size[0], image_size[1], image_size[2])\n",
    "\n",
    "        self.start = double_conv(\n",
    "            in_channels=1,\n",
    "            intermediate_channels=image_size[0],\n",
    "            out_channels=image_size[0],\n",
    "        )\n",
    "        self.down1 = double_conv(\n",
    "            in_channels=image_size[0],\n",
    "            intermediate_channels=image_size[0] * 2,\n",
    "            out_channels=image_size[0] * 2,\n",
    "        )\n",
    "        self.down2 = double_conv(\n",
    "            in_channels=image_size[0] * 2,\n",
    "            intermediate_channels=image_size[0] * 4,\n",
    "            out_channels=image_size[0] * 4,\n",
    "        )\n",
    "        self.bridge = double_conv(\n",
    "            in_channels=image_size[0] * 4,\n",
    "            intermediate_channels=image_size[0] * 8,\n",
    "            out_channels=image_size[0] * 4,\n",
    "        )\n",
    "        self.up2 = double_conv(\n",
    "            in_channels=image_size[0] * 8,\n",
    "            intermediate_channels=image_size[0] * 4,\n",
    "            out_channels=image_size[0] * 2,\n",
    "        )\n",
    "        self.up1 = double_conv(\n",
    "            in_channels=image_size[0] * 4,\n",
    "            intermediate_channels=image_size[0] * 2,\n",
    "            out_channels=image_size[0],\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            *conv(in_channels=image_size[0] * 2, out_channels=image_size[0]),\n",
    "            nn.Conv3d(in_channels=image_size[0], out_channels=1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        results: list[Tensor] = [self.start(x)]\n",
    "        results.append(self.down1(F.max_pool3d(results[-1], 2)))\n",
    "        results.append(self.down2(F.max_pool3d(results[-1], 2)))\n",
    "\n",
    "        x = F.interpolate(\n",
    "            self.bridge(F.max_pool3d(results[-1], 2)), size=results[-1].shape[2:]\n",
    "        )\n",
    "        x = F.interpolate(\n",
    "            self.up2(torch.cat((x, results[-1]), dim=1)), size=results[-2].shape[2:]\n",
    "        )\n",
    "        x = F.interpolate(\n",
    "            self.up1(torch.cat((x, results[-2]), dim=1)), size=results[-3].shape[2:]\n",
    "        )\n",
    "        x = self.final(torch.cat((x, results[-3]), dim=1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "model = UNet(image_size=output_size)\n",
    "first_image = train_data[0][0].unsqueeze(0)\n",
    "batched_images = torch.stack([first_image, first_image])\n",
    "\n",
    "predictions = model(batched_images)\n",
    "print(predictions.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
