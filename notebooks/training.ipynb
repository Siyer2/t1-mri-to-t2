{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nibabel matplotlib numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"Convert images in sample to Tensors\"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self, sample: tuple[np.ndarray, np.ndarray]\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        src, tgt = sample\n",
    "        return torch.from_numpy(src).float(), torch.from_numpy(tgt).float()\n",
    "\n",
    "\n",
    "class RandomCrop3D:\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (tuple))\n",
    "        assert len(output_size) == 3\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[0], sample[1]\n",
    "\n",
    "        h, w, d = image.shape[:3]\n",
    "        new_h, new_w, new_d = self.output_size\n",
    "\n",
    "        if new_h > h or new_w > w or new_d > d:\n",
    "            raise ValueError(\"Output size is larger than input dimensions.\")\n",
    "\n",
    "        top = np.random.randint(0, h - new_h + 1)\n",
    "        left = np.random.randint(0, w - new_w + 1)\n",
    "        depth = np.random.randint(0, d - new_d + 1)\n",
    "\n",
    "        image = image[top : top + new_h, left : left + new_w, depth : depth + new_d]\n",
    "        label = label[top : top + new_h, left : left + new_w, depth : depth + new_d]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "is_kaggle = True if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") else False\n",
    "route_to_small = \"/kaggle/input/t1t2-mri-data/small\" if is_kaggle else \"../small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, source_dir: str, target_dir: str, transforms=None):\n",
    "        self.source_images = [\n",
    "            self._get_image_data(\"t1\", image)\n",
    "            for image in sorted(os.listdir(source_dir))\n",
    "        ]\n",
    "        self.target_images = [\n",
    "            self._get_image_data(\"t2\", image)\n",
    "            for image in sorted(os.listdir(target_dir))\n",
    "        ]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_images)\n",
    "\n",
    "    def _get_image_data(self, image_type: str, study_id: str):\n",
    "        image = nib.load(f\"{route_to_small}/{image_type}/{study_id}\")\n",
    "        return image.get_fdata()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_data = (self.source_images[idx], self.target_images[idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            image_data = self.transforms(image_data)\n",
    "\n",
    "        return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "\n",
    "output_size = (64, 64, 32)\n",
    "dataset = NiftiDataset(\n",
    "    source_dir=f\"{route_to_small}/t1\",\n",
    "    target_dir=f\"{route_to_small}/t2\",\n",
    "    transforms=Compose([RandomCrop3D(output_size=output_size), ToTensor()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = 0.1\n",
    "batch_size = 16\n",
    "num_jobs = 12\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(valid_split * num_train)\n",
    "valid_idx = np.random.choice(indices, size=split, replace=False)\n",
    "train_idx = list(set(indices) - set(valid_idx))\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_jobs,\n",
    "    pin_memory=True,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=valid_sampler,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_jobs,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def conv(in_channels: int, out_channels: int):\n",
    "    return (\n",
    "        nn.Conv3d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        ),\n",
    "        nn.BatchNorm3d(num_features=out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "def double_conv(in_channels: int, intermediate_channels: int, out_channels: int):\n",
    "    return nn.Sequential(\n",
    "        *conv(in_channels=in_channels, out_channels=intermediate_channels),\n",
    "        *conv(in_channels=intermediate_channels, out_channels=out_channels)\n",
    "    )\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_size: tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "\n",
    "        # input shape = (1, image_size[0], image_size[1], image_size[2])\n",
    "\n",
    "        self.start = double_conv(\n",
    "            in_channels=1,\n",
    "            intermediate_channels=image_size[0],\n",
    "            out_channels=image_size[0],\n",
    "        )\n",
    "        self.down1 = double_conv(\n",
    "            in_channels=image_size[0],\n",
    "            intermediate_channels=image_size[0] * 2,\n",
    "            out_channels=image_size[0] * 2,\n",
    "        )\n",
    "        self.down2 = double_conv(\n",
    "            in_channels=image_size[0] * 2,\n",
    "            intermediate_channels=image_size[0] * 4,\n",
    "            out_channels=image_size[0] * 4,\n",
    "        )\n",
    "        self.bridge = double_conv(\n",
    "            in_channels=image_size[0] * 4,\n",
    "            intermediate_channels=image_size[0] * 8,\n",
    "            out_channels=image_size[0] * 4,\n",
    "        )\n",
    "        self.up2 = double_conv(\n",
    "            in_channels=image_size[0] * 8,\n",
    "            intermediate_channels=image_size[0] * 4,\n",
    "            out_channels=image_size[0] * 2,\n",
    "        )\n",
    "        self.up1 = double_conv(\n",
    "            in_channels=image_size[0] * 4,\n",
    "            intermediate_channels=image_size[0] * 2,\n",
    "            out_channels=image_size[0],\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            *conv(in_channels=image_size[0] * 2, out_channels=image_size[0]),\n",
    "            nn.Conv3d(in_channels=image_size[0], out_channels=1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if x.dim() == 4:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        results: list[Tensor] = [self.start(x)]\n",
    "        results.append(self.down1(F.max_pool3d(results[-1], 2)))\n",
    "        results.append(self.down2(F.max_pool3d(results[-1], 2)))\n",
    "\n",
    "        x = F.interpolate(\n",
    "            self.bridge(F.max_pool3d(results[-1], 2)), size=results[-1].shape[2:]\n",
    "        )\n",
    "        x = F.interpolate(\n",
    "            self.up2(torch.cat((x, results[-1]), dim=1)), size=results[-2].shape[2:]\n",
    "        )\n",
    "        x = F.interpolate(\n",
    "            self.up1(torch.cat((x, results[-2]), dim=1)), size=results[-3].shape[2:]\n",
    "        )\n",
    "        x = self.final(torch.cat((x, results[-3]), dim=1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(image_size=output_size).to(device=device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=1e-6)\n",
    "criterion = nn.SmoothL1Loss()  # nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('trained.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_train_losses, epoch_val_losses = [], []\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "for t in range(1, num_epochs + 1):\n",
    "    # training\n",
    "    train_losses = []\n",
    "    model.train(True)\n",
    "    for i, (source, target) in enumerate(train_loader):\n",
    "        source, target = source.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(source)\n",
    "        loss = criterion(out, target)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_train_losses.append(train_losses)\n",
    "\n",
    "    # validation\n",
    "    val_losses = []\n",
    "    model.train(False)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for source, target in valid_loader:\n",
    "            source, target = source.to(device), target.to(device)\n",
    "            out = model(source)\n",
    "            loss = criterion(out, target)\n",
    "            val_losses.append(loss.item())\n",
    "        epoch_val_losses.append(val_losses)\n",
    "\n",
    "    if not np.all(np.isfinite(train_losses)):\n",
    "        raise RuntimeError(\"NaN or Inf in training loss, cannot recover. Exiting.\")\n",
    "    log = f\"Epoch: {t} - Training Loss: {np.mean(train_losses):.2e}, Validation Loss: {np.mean(val_losses):.2e}\"\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trained.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
